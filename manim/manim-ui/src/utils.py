import re

SYS_PROMPT_APPEND = """Write Manim scripts for animations in Python. Generate code, not text. Never explain code. Never add functions. Never add comments. Never infinte loops. Use variables with length of maximum 2-3 characters. Do all imports other than math/manim inside the construct function. 
```
from manim import *
from math import *

class GenScene(Scene):
    def construct(self):
        # Complete code here
```"""

# Define a function to query GPT-4
def query_gpt(client, model_name, history=None, stream=False):
  response = client.chat.completions.create(
    model=model_name,
    messages=[
      {"role": m['role'], "content": m['content'] + ' ' + SYS_PROMPT_APPEND if i == len(history)-1 else m['content']} for i, m in enumerate(history)
    ],
    stream=stream,
  )
  return response

# Define a function to query Claude-3
def query_claude(client, model_name, history=None, stream=False):
  response = client.messages.create(
      model=model_name,
      max_tokens=2048,
      messages=[
        {"role": m['role'], "content": m['content'] + ' ' + SYS_PROMPT_APPEND if i == len(history)-1 else m['content']} for i, m in enumerate(history)
      ],
      stream=stream,
  )
  return response

def claude_stream_to_generator(stream):
  for event in stream:
    if event.type == "content_block_delta":
      yield event.delta.text

def query_llm():
  new_user_input_ids = tokenizer.encode(tokenizer.eos_token + input_text,
                                        return_tensors='pt')

  bot_input_ids = torch.cat([history,
                              new_user_input_ids],
                              dim=-1) if history is not None else new_user_input_ids

  # generated a response while limiting the total chat history to 1000 tokens,
  chat_history_ids = model.generate(bot_input_ids,
                                    max_length=generate_size,
                                    pad_token_id=tokenizer.eos_token_id)

  return tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0],
                          skip_special_tokens=True), chat_history_ids

def wrap_prompt(prompt: str) -> str:
  """
    Wraps the prompt in the LLM instructions
  """
  return f"Animation Request: {prompt}"

def extract_code(text: str) -> str:
  """
    Extracts the code from the text generated by LLM from the ``` ``` blocks
  """
  pattern = re.compile(r"```(.*?)```", re.DOTALL)
  match = pattern.search(text)
  if match:
    return match.group(1).strip()
  else:
    return text

def extract_construct_code(code_str: str) -> str:
  """
    Extracts the code from the construct method
  """
  pattern = r"def construct\(self\):([\s\S]*)"
  match = re.search(pattern, code_str)
  if match:
    return match.group(1)
  else:
    return ""


def create_file_content(code_response: str, command: str) -> str:
  """
    Creates the content of the file to be written
  """
  return f"""# Manim code generated from LLM
# {command}

from manim import *
from math import *

class GenScene(Scene):
    def construct(self):
{code_response}"""
